{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a8f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Geospatial\n",
    "import contextily as cx\n",
    "import xarray as xr\n",
    "import zarr # Not referenced, but required for xarray\n",
    "\n",
    "# Import Planetary Computer tools\n",
    "import fsspec\n",
    "import pystac\n",
    "\n",
    "# Other\n",
    "import os\n",
    "import zipfile\n",
    "from itertools import cycle\n",
    "\n",
    "# Path to data folder with provided material\n",
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_path+'training_data/'):\n",
    "    os.mkdir(data_path+'training_data/')\n",
    "    with zipfile.ZipFile(data_path+'GBIF_training_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_path+'training_data/')\n",
    "        \n",
    "def filter_bbox(frogs, bbox):\n",
    "    frogs = frogs[lambda x: \n",
    "        (x.decimalLongitude >= bbox[0]) &\n",
    "        (x.decimalLatitude >= bbox[1]) &\n",
    "        (x.decimalLongitude <= bbox[2]) &\n",
    "        (x.decimalLatitude <= bbox[3])\n",
    "    ]\n",
    "    return frogs\n",
    "\n",
    "def get_frogs(file, year_range=None, bbox=None):\n",
    "    \"\"\"Returns the dataframe of all frog occurrences for the bounding box specified.\"\"\"\n",
    "    columns = [\n",
    "        'gbifID','eventDate','country','continent','stateProvince',\n",
    "        'decimalLatitude','decimalLongitude','species', 'coordinateUncertaintyInMeters'\n",
    "    ]\n",
    "    country_names = {\n",
    "        'AU':'Australia', 'CR':'Costa Rica', 'ZA':'South Africa','MX':'Mexico','HN':'Honduras',\n",
    "        'MZ':'Mozambique','BW':'Botswana','MW':'Malawi','CO':'Colombia','PA':'Panama','NI':'Nicaragua',\n",
    "        'BZ':'Belize','ZW':'Zimbabwe','SZ':'Eswatini','ZM':'Zambia','GT':'Guatemala','LS':'Lesotho',\n",
    "        'SV':'El Salvador', 'AO':'Angola', np.nan:'unknown or invalid'\n",
    "    }\n",
    "    continent_names = {\n",
    "        'AU':'Australia', 'CR':'Central America', 'ZA':'Africa','MX':'Central America','HN':'Central America',\n",
    "        'MZ':'Africa','BW':'Africa','MW':'Africa','CO':'Central America','PA':'Central America',\n",
    "        'NI':'Central America','BZ':'Central America','ZW':'Africa','SZ':'Africa','ZM':'Africa',\n",
    "        'GT':'Central America','LS':'Africa','SV':'Central America','AO':'Africa', np.nan:'unknown or invalid' \n",
    "    }\n",
    "    frogs = (\n",
    "        pd.read_csv(data_path+'training_data/occurrence.txt', sep='\\t', parse_dates=['eventDate'])\n",
    "        .assign(\n",
    "            country =  lambda x: x.countryCode.map(country_names),\n",
    "            continent =  lambda x: x.countryCode.map(continent_names),\n",
    "            species = lambda x: x.species.str.title()\n",
    "        )\n",
    "        [columns]\n",
    "    )\n",
    "    if year_range is not None:\n",
    "        frogs = frogs[lambda x: \n",
    "            (x.eventDate.dt.year >= year_range[0]) & \n",
    "            (x.eventDate.dt.year <= year_range[1])\n",
    "        ]\n",
    "    if bbox is not None:\n",
    "        frogs = filter_bbox(frogs, bbox)\n",
    "    return frogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc51d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bounding box for Australia Region of Interest\n",
    "region_name = 'Greater Sydney, NSW'\n",
    "# min_lon, min_lat = (150.15, -34.25)  # Lower-left corner\n",
    "# max_lon, max_lat = (151.15, -33.25)  # Upper-right corner\n",
    "min_lon, min_lat = (115, -40.00)  # Lower-left corner\n",
    "max_lon, max_lat = (154.00, -10.00)  # Upper-right corner\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "#year_range_list = [(2015, 2019)]\n",
    "bbox_range_list = [(144.8,-38.5,145.8,-37.5), (150.7,-33.5,151.7,-32.5), (152.6,-29.0,153.6,-28.0),\n",
    "              (145.0,-17.7,146.0,-16.7), (115.7,-32.5,116.7,-31.5)]\n",
    "\n",
    "all_frog_data_dict = {}\n",
    "year_range_list = [(2014, 2015), (2016, 2017), (2018, 2019)]\n",
    "\n",
    "# Load in data\n",
    "for year_range in year_range_list:\n",
    "    all_frog_data = get_frogs(data_path+'/training_data/occurrence.txt', year_range=year_range, bbox=bbox)\n",
    "    all_frog_data_dict[year_range] = all_frog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d540999",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_species = 'Litoria Fallax'\n",
    "\n",
    "for key, all_frog_data in all_frog_data_dict.items():\n",
    "    all_frog_data = (\n",
    "        all_frog_data\n",
    "        # Assign the occurrenceStatus to 1 for the target species and 0 for all other species.\n",
    "        # as well as a key for joining (later)\n",
    "        .reset_index(drop = True)\n",
    "        .assign(\n",
    "            occurrenceStatus = lambda x: np.where(x.species == target_species, 1, 0),\n",
    "            key = lambda x: x.index\n",
    "        )\n",
    "    )\n",
    "    all_frog_data['coordinateUncertaintyInMeters'] = all_frog_data['coordinateUncertaintyInMeters'].fillna(0)\n",
    "    all_frog_data = all_frog_data[all_frog_data['coordinateUncertaintyInMeters'] <= 100]\n",
    "    \n",
    "    # all_frog_data['season'] = all_frog_data['eventDate'].dt.quarter\n",
    "    # all_frog_data['month'] = all_frog_data['eventDate'].dt.month\n",
    "    # all_frog_data['week'] = all_frog_data['eventDate'].dt.week\n",
    "    \n",
    "    all_frog_data_dict[key] = all_frog_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3215e4e",
   "metadata": {},
   "source": [
    "# TerraClimate Data (18 Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = pystac.read_file(\"https://planetarycomputer.microsoft.com/api/stac/v1/collections/terraclimate\")\n",
    "asset = collection.assets[\"zarr-https\"]\n",
    "store = fsspec.get_mapper(asset.href)\n",
    "data = xr.open_zarr(store, **asset.extra_fields[\"xarray:open_kwargs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec35fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terraclimate(bbox, metrics, time_slice=None, assets=None, features=None, interp_dims=None, verbose=True):\n",
    "    \"\"\"Returns terraclimate metrics for a given area, allowing results to be interpolated onto a larger image.\n",
    "    \n",
    "    Attributes:\n",
    "    bbox -- Tuple of (min_lon, min_lat, max_lon, max_lat) to define area\n",
    "    metrics -- Nested dictionary in the form {<metric_name>:{'fn':<metric_function>,'params':<metric_kwargs_dict>}, ... }\n",
    "    time_slice -- Tuple of datetime strings to select data between, e.g. ('2015-01-01','2019-12-31')\n",
    "    assets -- list of terraclimate assets to take\n",
    "    features -- list of asset metrics to take, specified by strings in the form '<asset_name>_<metric_name>'\n",
    "    interp_dims -- Tuple of dimensions (n, m) to interpolate results to\n",
    "    \"\"\"\n",
    "    min_lon, min_lat, max_lon, max_lat = bbox\n",
    "    \n",
    "    collection = pystac.read_file(\"https://planetarycomputer.microsoft.com/api/stac/v1/collections/terraclimate\")\n",
    "    asset = collection.assets[\"zarr-https\"]\n",
    "    store = fsspec.get_mapper(asset.href)\n",
    "    data = xr.open_zarr(store, **asset.extra_fields[\"xarray:open_kwargs\"])\n",
    "    \n",
    "    # Select datapoints that overlap region\n",
    "    if time_slice is not None:\n",
    "        data = data.sel(lon=slice(min_lon,max_lon),lat=slice(max_lat,min_lat),time=slice(time_slice[0],time_slice[1]))\n",
    "    else:\n",
    "        data = data.sel(lon=slice(min_lon,max_lon),lat=slice(max_lat,min_lat))\n",
    "    if assets is not None:\n",
    "        data = data[assets]\n",
    "    print('Loading data') if verbose else None\n",
    "    data = data.rename(lat='y', lon='x').to_array().compute()\n",
    "    \n",
    "    print(f'Data Shape: {data.shape}')\n",
    "        \n",
    "    # Calculate metrics\n",
    "    combined_values = []\n",
    "    combined_bands = []\n",
    "    for name, metric in metrics.items():\n",
    "        print(f'Calculating {name}') if verbose else None\n",
    "        sum_data = xr.apply_ufunc(\n",
    "            metric['fn'], data, input_core_dims=[[\"time\"]], kwargs=metric['params'], dask = 'allowed', vectorize = True\n",
    "        ).rename(variable='band')\n",
    "        \n",
    "        xcoords = sum_data.x\n",
    "        ycoords = sum_data.y\n",
    "        dims = sum_data.dims\n",
    "        print(f'Dimensions : {dims}')\n",
    "        # print(f'Sum_data values {sum_data.values}')\n",
    "        combined_values.append(sum_data.values)\n",
    "        for band in sum_data.band.values:\n",
    "            combined_bands.append(band+'_'+name)\n",
    "    \n",
    "    # Combine metrics\n",
    "    combined_values = np.concatenate(\n",
    "        combined_values,\n",
    "        axis=0\n",
    "    )\n",
    "    combined_data = xr.DataArray(\n",
    "        data=combined_values,\n",
    "        dims=dims,\n",
    "        coords=dict(\n",
    "            band=combined_bands,\n",
    "            y=ycoords,\n",
    "            x=xcoords\n",
    "        )\n",
    "    )    \n",
    "\n",
    "    # Take relevant bands:\n",
    "    combined_data = combined_data.sel(band=features)\n",
    "    \n",
    "    if interp_dims is not None:\n",
    "        print(f'Interpolating image') if verbose else None\n",
    "        interp_coords = (np.linspace(bbox[0], bbox[2], interp_dims[0]), np.linspace(bbox[1], bbox[3], interp_dims[1]))\n",
    "        combined_data = combined_data.interp(x=interp_coords[0], y=interp_coords[1], method='nearest', kwargs={\"fill_value\": \"extrapolate\"})\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to measure over time dimension\n",
    "tc_metrics = {\n",
    "    'mean':{\n",
    "        'fn':np.nanmean,\n",
    "        'params':{}\n",
    "    },\n",
    "    'std':{\n",
    "        'fn':np.nanstd,\n",
    "        'params':{}\n",
    "    },\n",
    "    'median':{\n",
    "        'fn':np.nanmedian,\n",
    "        'params':{}\n",
    "    },\n",
    "    'min':{\n",
    "        'fn':np.nanmax,\n",
    "        'params':{}\n",
    "    },\n",
    "    'max':{\n",
    "        'fn':np.nanmin,\n",
    "        'params':{}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Date range to take\n",
    "time_slice = ('2014-01-01','2019-12-31')\n",
    "\n",
    "# Measurements to take\n",
    "assets = ['aet', 'def', 'pdsi', 'pet', 'ppt', 'ppt_station_influence', 'q', 'soil', 'srad', 'swe', 'tmax',\n",
    "         'tmax_station_influence', 'tmin', 'tmin_station_influence', 'vap', 'vap_station_influence', 'vpd', 'ws']\n",
    "\n",
    "\n",
    "features = ['aet_mean', 'def_mean', 'pdsi_mean', 'pet_mean', 'ppt_mean', 'ppt_station_influence_mean', 'q_mean',\n",
    "            'soil_mean', 'srad_mean', 'swe_mean', 'tmax_mean', 'tmax_station_influence_mean', 'tmin_mean',\n",
    "            'tmin_station_influence_mean', 'vap_mean', 'vap_station_influence_mean', 'vpd_mean', 'ws_mean',\n",
    "           'aet_std', 'def_std', 'pdsi_std', 'pet_std', 'ppt_std', 'ppt_station_influence_std', 'q_std',\n",
    "            'soil_std', 'srad_std', 'swe_std', 'tmax_std', 'tmax_station_influence_std', 'tmin_std',\n",
    "            'tmin_station_influence_std', 'vap_std', 'vap_station_influence_std', 'vpd_std', 'ws_std',\n",
    "           'aet_min', 'def_min', 'pdsi_min', 'pet_min', 'ppt_min', 'ppt_station_influence_min', 'q_min', 'soil_min',\n",
    "            'srad_min', 'swe_min', 'tmax_min', 'tmax_station_influence_min', 'tmin_min', 'tmin_station_influence_min',\n",
    "            'vap_min', 'vap_station_influence_min', 'vpd_min', 'ws_min',\n",
    "           'aet_max', 'def_max', 'pdsi_max', 'pet_max', 'ppt_max', 'ppt_station_influence_max', 'q_max', 'soil_max',\n",
    "            'srad_max', 'swe_max', 'tmax_max', 'tmax_station_influence_max', 'tmin_max', 'tmin_station_influence_max',\n",
    "            'vap_max', 'vap_station_influence_max', 'vpd_max', 'ws_max',\n",
    "           'aet_median', 'def_median', 'pdsi_median', 'pet_median', 'ppt_median', 'ppt_station_influence_median', 'q_median',\n",
    "            'soil_median', 'srad_median', 'swe_median', 'tmax_median', 'tmax_station_influence_median', 'tmin_median',\n",
    "            'tmin_station_influence_median', 'vap_median', 'vap_station_influence_median', 'vpd_median', 'ws_median']\n",
    "\n",
    "weather_data_dict = {}\n",
    "time_slice_list = [('2014-01-01','2015-12-31'),\n",
    "                  ('2016-01-01','2017-12-31'), ('2018-01-01','2019-12-31')]\n",
    "\n",
    "for time_slice in time_slice_list:\n",
    "    weather_data = get_terraclimate(bbox, tc_metrics, time_slice=time_slice, assets=assets, features=features)\n",
    "    display(weather_data.band.values)\n",
    "    \n",
    "    weather_data_dict[time_slice] = weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61db673",
   "metadata": {},
   "source": [
    "# Merging Frog Data with Weather Data using \"key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec48e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_frogs(frogs, data):\n",
    "    \"\"\"Collects the data for each frog location and joins it onto the frog data \n",
    "\n",
    "    Arguments:\n",
    "    frogs -- dataframe containing the response variable along with [\"decimalLongitude\", \"decimalLatitude\", \"key\"]\n",
    "    data -- xarray dataarray of features, indexed with geocoordinates\n",
    "    \"\"\"\n",
    "    return frogs.merge(\n",
    "        (\n",
    "            data\n",
    "            .rename('data')\n",
    "            .sel(\n",
    "                x=xr.DataArray(all_frog_data.decimalLongitude, dims=\"key\", coords={\"key\": all_frog_data.key}), \n",
    "                y=xr.DataArray(all_frog_data.decimalLatitude, dims=\"key\", coords={\"key\": all_frog_data.key}),\n",
    "                method=\"nearest\"\n",
    "            )\n",
    "            .to_dataframe()\n",
    "            .assign(val = lambda x: x.iloc[:, -1])\n",
    "            [['val']]\n",
    "            .reset_index()\n",
    "            .drop_duplicates()\n",
    "            .pivot(index=\"key\", columns=\"band\", values=\"val\")\n",
    "            .reset_index()\n",
    "        ),\n",
    "        on = ['key'],\n",
    "        how = 'inner'\n",
    "    )\n",
    "\n",
    "all_model_data = []\n",
    "\n",
    "for all_frog_data, weather_data in zip(all_frog_data_dict.values(), weather_data_dict.values()):\n",
    "    \n",
    "    print(f'Frog data shape: {all_frog_data.shape}')\n",
    "    print(f'Weather data shape: {weather_data.shape}')\n",
    "    model_data = join_frogs(all_frog_data, weather_data)\n",
    "    print(f'After merging shape: {model_data.shape}')\n",
    "    \n",
    "    all_model_data.append(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9357c0",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef0bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_model_data[0]\n",
    "\n",
    "for i in range(1, len(all_model_data)):\n",
    "    df = pd.concat([df, all_model_data[i]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b635eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['ws_max'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24064f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1178dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (\n",
    "    model_data\n",
    "    .drop(['gbifID', 'eventDate', 'decimalLatitude', 'decimalLongitude', 'species', 'coordinateUncertaintyInMeters',\n",
    "       'stateProvince', 'country', 'continent', 'occurrenceStatus', 'key'], 1)\n",
    ")\n",
    "\n",
    "y = model_data.occurrenceStatus.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd248f2",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#from imblearn.under_sampling import TomekLinks\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81787f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# tl = TomekLinks(sampling_strategy = 'majority')\n",
    "# X_train, y_train = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=123)\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5360b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params_random_search = {\n",
    "    'learning_rate': np.arange(0.01, 1.01, 0.01),\n",
    "    'n_estimators': np.arange(500, 2000),\n",
    "    'max_depth': range(2, 5),\n",
    "    'subsample': np.arange(0.02, 1.02, 0.02),\n",
    "    'colsample_bytree': np.arange(0.3, 0.7, 0.1),\n",
    "    'scale_pos_weight': np.arange(1, 3, 0.1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ee746",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_cv = RandomizedSearchCV(estimator=xg_cl, param_distributions=params_random_search, scoring='roc_auc', n_iter=10,\n",
    "                    cv=5, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a1e35",
   "metadata": {},
   "source": [
    "**Free free to run the 2 code blocks below for hyperparmeter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized_cv.fit(X_train, y_train)\n",
    "# print(\"RandomizedSearchCV\")\n",
    "# print(\"Best parameters found: \", randomized_cv.best_params_)\n",
    "# print(\"Best ROC AUC found: \", randomized_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98003d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', subsample = 0.84, n_estimators=1102,\n",
    "         max_depth=3, learning_rate=0.14, colsample_bytree=0.4, scale_pos_weight=1.3, seed=123, importance_type = 'gain')\n",
    "         \n",
    "xg_cl.fit(X_train, y_train, eval_metric=[\"error\"], eval_set=eval_set, verbose=True)\n",
    "results = xg_cl.evals_result()\n",
    "predictions = xg_cl.predict(X_test)\n",
    "print(f\"F1 Score: {np.mean(f1_score(y_test, predictions)).round(2)}\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_score(y_test, predictions)).round(2)}\")\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# plot classification error\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, epochs)\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82941bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', subsample = 0.84, n_estimators=1102,\n",
    "         max_depth=3, learning_rate=0.14, colsample_bytree=0.4, scale_pos_weight=1.3, seed=123, importance_type = 'gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_cl.fit(X, y, eval_metric=[\"error\"], eval_set=[(X, y)], verbose=True)\n",
    "results = xg_cl.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b03359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "# plot classification error\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, epochs)\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72586460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "plot_importance(xg_cl, max_num_features=20, importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "features = X.columns\n",
    "importances = xg_cl.feature_importances_\n",
    "\n",
    "temp = pd.DataFrame(data={'feature_names': features, 'importance_scores': importances}).sort_values(['importance_scores'], \n",
    "                                                                                                    ascending=False).reset_index(drop=True)\n",
    "temp[:30]['feature_names'].values\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7634d21",
   "metadata": {},
   "source": [
    "# Save model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b776291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#pickle.dump(xg_cl, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f9097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
